{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 6 - movie poster classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label (meaning that there is more than one \"true\" label) classification of movie poster images by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "I have left this part mostly (if not totally) unchanged. The data loading part does not require any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import transform\n",
    "from skimage import color\n",
    "\n",
    "class MoviePosterDataset(Dataset):\n",
    "    def __init__(self, csvfile, imagedir, device=\"cpu\"):\n",
    "        self.posterlist = pd.read_csv(csvfile)\n",
    "        self.imagedir = imagedir\n",
    "        \n",
    "        imageids = list(self.posterlist[\"Id\"])\n",
    "        imagefiles = [\"{}/{}.jpg\".format(self.imagedir, x) for x in imageids]\n",
    "        images = [np.array(io.imread(x)) for x in imagefiles]\n",
    "        images = np.array([color.gray2rgb(x) if len(x.shape) < 3 else x for x in images])\n",
    "        \n",
    "        truths = self.posterlist[self.posterlist.columns[2:]]\n",
    "        self.truths = torch.Tensor(truths.to_numpy())\n",
    "    \n",
    "        tns = torch.from_numpy(images)\n",
    "        self.images = tns.permute(0, 3, 1, 2)\n",
    "        \n",
    "        if device != \"cpu\":\n",
    "            self.device = torch.device(device)\n",
    "            self.images = self.images.to(self.device)\n",
    "            self.truths = self.truths.to(self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.posterlist)\n",
    "    \n",
    "    def __getitem__(self, idx):            \n",
    "        truths = self.truths[idx]\n",
    "        images = self.images[idx]\n",
    "        \n",
    "        return images, truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed the paths here, and specified the path to the training file and the dataset of images as it is on the server. I did not load anything on the GPU as I want to use that only while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7254, 3, 300, 450]), torch.Size([7254, 25]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd = MoviePosterDataset(\"/usr/local/courses/lt2316-h19/Multi_Label_dataset/train.csv\", \n",
    "                         \"/usr/local/courses/lt2316-h19/Multi_Label_dataset/ImageSmaller\", device=\"cpu\")\n",
    "mpd.images.shape, mpd.truths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "I spent a lot of time on this model, experimenting with different layers and network architecture elements. The code looks quite messy here, but I did not clean it up because I wanted the commented out code to reflect the various things I tried out before eventually deciding upon said architecture. To make it more readable and understandable, here is the final network architecture that I'm using:\n",
    "\n",
    "- A 2d convolutional layer with 3 input channels (this is the input image), 16 output channels, a filter size of 5, a stride of 1 and padding of 2. I used this particular configuration (especially the padding value) so that the resultant image after applying the convolution remains of the same dimensions.\n",
    "- Batch Normalization\n",
    "- Activation function ReLU\n",
    "- Maxpooling with filter size 2 and stride of 2. This reduces the image dimensions to (150, 225).\n",
    "\n",
    "\n",
    "- A 2d convolutional layer with 16 input channels (from the previous convolution), 32 output channels, a filter size of 5, a stride of 1 and padding of (2,3). \n",
    "- Dropout of 0.2\n",
    "- Batch Normalization\n",
    "- Activation function ReLU\n",
    "- Maxpooling with filter size 2 and stride of 2. This reduces the image dimensions to (75, 112).\n",
    "\n",
    "\n",
    "- A 2d convolutional layer with 32 input channels (from the previous convolution), 64 output channels, a filter size of 5, a stride of 1 and padding of (3,2). \n",
    "- Dropout of 0.2\n",
    "- Batch Normalization\n",
    "- Activation function ReLU\n",
    "- Maxpooling with filter size 2 and stride of 2. This reduces the image dimensions to (37, 56).\n",
    "\n",
    "\n",
    "- A linear layer which reshapes the output from 64\\*37\\*56 to 3000 data points.\n",
    "- A dropout of 0.2\n",
    "- Activation function ReLU\n",
    "\n",
    "\n",
    "- A linear layer which reshapes the output from 3000 to 25 data points.\n",
    "- Dropout of 0.2\n",
    "- Activation function Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with the model\n",
    "\n",
    "I noticed that having only one convolutional layer doesn't do much - the model does not really learn anything. Adding two didn't help much either - it was with 3 convolutional layers that the training loss started going down consistently. Having a fourth convolutional layer helps even more, and the model converges faster - however, this fourth convolutional layer also makes the model overfit more.\n",
    "\n",
    "And this is indeed the more troubling issue that I've faced with this task - the loss on the validation loss goes down in the start, but then just starts fluctuating around a fixed value. Moreover, if the number of epochs is high, the validation loss eventually starts increasing, which I think indicates that the model is overfitting. Because of this, I removed the fourth convolutional layer.\n",
    "\n",
    "I also had another linear layer, between the final two linear layers, but I removed it for the same reason - the model was overfitting on the data. \n",
    "\n",
    "For dealing with overfitting, I found that batch normalization helped a lot, as it normalizes the data between the layers. I added dropouts too, but I found they do not help as much - and if I add a large value for the dropout, the model stops learning at all - which makes sense I suppose.\n",
    "\n",
    "The output channels I initially had were 5, 10, 15... but later I realized that the convention is to use output channels of values to the power of 2. So then I started with 16, then went to 32 and 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosterClassifier(nn.Module):\n",
    "    def __init__(self, d1, d2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        in_channels = 3\n",
    "#         self.out_channels = 5\n",
    "        filter_size = 5\n",
    "        padding = 2\n",
    "        stride = 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, filter_size, stride=stride, padding=padding)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "        self.bnm1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, filter_size, stride=stride, padding=padding)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.bnm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d((2,3), stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, filter_size, stride=stride, padding=padding)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.bnm3 = nn.BatchNorm2d(64)\n",
    "        self.maxpool3 = nn.MaxPool2d((3,2), stride=2)\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(64, 128, filter_size, stride=stride, padding=padding)\n",
    "#         self.dropout4 = nn.Dropout(dropout)\n",
    "#         self.bnm4 = nn.BatchNorm2d(128)\n",
    "#         self.maxpool4 = nn.MaxPool2d((3,2), stride=2)\n",
    "        \n",
    "#         self.conv2d1 = nn.Conv2d(in_channels, self.out_channels, filter_size, stride=stride, padding=padding)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "#         self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "#         self.conv2d2 = nn.Conv2d(self.out_channels, 10, filter_size, stride=stride, padding=padding)\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "#         self.maxpool2 = nn.MaxPool2d((2,3), stride=2)\n",
    "        \n",
    "#         self.conv2d3 = nn.Conv2d(10, 32, filter_size, stride=stride, padding=padding)\n",
    "#         self.dropout3 = nn.Dropout(dropout)\n",
    "#         self.maxpool3 = nn.MaxPool2d((3,2), stride=2)\n",
    "        \n",
    "        self.linear0 = nn.Linear(64 * 37 * 56, 3000)\n",
    "#         self.linear0 = nn.Linear(128 * 18 * 28, 3000)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.linear1 = nn.Linear(3000, 1500)\n",
    "#         self.dropout5 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear2 = nn.Linear(3000, 25)\n",
    "        self.dropout6 = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.bnm1(output)\n",
    "#         output = self.dropout1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.maxpool1(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.dropout2(output)\n",
    "        output = self.bnm2(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.maxpool2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.bnm3(output)\n",
    "        output = self.dropout3(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.maxpool3(output)\n",
    "        \n",
    "#         output = self.conv4(output)\n",
    "#         output = self.bnm4(output)\n",
    "#         output = self.dropout4(output)\n",
    "#         output = nn.functional.relu(output)\n",
    "#         output = self.maxpool4(output)\n",
    "        \n",
    "        output = output.view(-1, 64 * 37 * 56)\n",
    "#         output = output.view(-1, 128 * 18 * 28)\n",
    "        output = self.linear0(output)\n",
    "        output = self.dropout4(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        \n",
    "#         output = self.linear1(output)\n",
    "#         output = self.dropout5(output)\n",
    "#         output = nn.functional.relu(output)\n",
    "        \n",
    "        output = self.linear2(output)\n",
    "        output = self.dropout6(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the data for the model\n",
    "\n",
    "This part originally had a 60/40 split for training and testing data. I have replaced this with a 60/20/20 split for training, validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "totalindices = list(range(len(mpd)))\n",
    "random.shuffle(totalindices)\n",
    "train_test_split_index = math.floor(len(mpd)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5803"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_indices = totalindices[:train_test_split_index]\n",
    "test_indices = totalindices[train_test_split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split_index = math.floor(len(train_val_indices)*0.8)\n",
    "train_indices = train_val_indices[:train_val_split_index]\n",
    "val_indices = train_val_indices[train_val_split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4642, 1161, 1451)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices), len(val_indices), len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size defined here is currently 32. I experimented with sized 16 and 32 during my work with the model and training. I originally wanted to have a batch size of 64, but that causes memory issues with the GPU, so I had to stick to either 16 or 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "traindl = torch.utils.data.DataLoader(mpd, batch_size=batch_size, \n",
    "                                      sampler=train_sampler, pin_memory=False)\n",
    "valdl = torch.utils.data.DataLoader(mpd, batch_size=batch_size, sampler=val_sampler, pin_memory=False)\n",
    "testdl = torch.utils.data.DataLoader(mpd, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run the training loop\n",
    "\n",
    "In the training loop here, I used the GPU \"cuda:1\", which is the GPU assigned to me as per my username. I used the Adam optimizer, with a learning rate of 0.0001 (as I found that the model starts oscillating around some value and doesn't converge if I use a higher learning rate), and added a weight decay of 1e-5 for some regularization. Most of the other configuration is kept the same.\n",
    "\n",
    "The major thing added here is the fact that in each epoch, I first set the model to train mode and train over all batches in the training dataloader. Then I set the model to eval mode, and test it over all batches in the validation dataloader. I calculate the total loss after both, and I maintain a list of train loss and validation loss after each epoch for plotting purposes later.\n",
    "\n",
    "I have set the number of epochs to 15 here because I found that if the model is trained over more epochs (like 30, as in the original example), the validation loss starts increasing, which means the model is overfitting. Since the model architecture is quite complex and overfitting can happen quickly, I found it best to limit the number of epochs to 10 or 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_index(y_true, pred):\n",
    "    intersection = 0\n",
    "    union = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == pred[i]:\n",
    "            intersection += 1\n",
    "        union += 1\n",
    "    j_index = float(intersection)/float(union)\n",
    "    return j_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(train_dataloader, val_dataloader, epochs=3):\n",
    "    dev = \"cuda:1\"\n",
    "    torch.cuda.empty_cache()\n",
    "    model = PosterClassifier(d1=450, d2=300)\n",
    "    model = model.to(dev)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        sum_train_loss = 0\n",
    "        sum_val_loss = 0\n",
    "        total_batches = 0\n",
    "        model.train()\n",
    "        for c, data in enumerate(train_dataloader):\n",
    "            images, truth = data\n",
    "            optimizer.zero_grad()\n",
    "            images = images.float().to(dev)\n",
    "            truth = truth.to(dev)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, truth)\n",
    "            sum_train_loss += loss.item()\n",
    "            total_batches += 1.0\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss = sum_train_loss/total_batches\n",
    "        train_losses.append(total_loss)\n",
    "        print(\"In epoch {}, training loss = {}\".format(epoch, total_loss))\n",
    "        \n",
    "        total_batches = 0\n",
    "        model.eval()\n",
    "        jaccard_indices = []\n",
    "        for c, data in enumerate(val_dataloader):\n",
    "            images, truth = data\n",
    "            images = images.float().to(dev)\n",
    "            truth = truth.to(dev)\n",
    "            output = model(images)\n",
    "            with torch.no_grad():\n",
    "                loss = criterion(output, truth)            \n",
    "            sum_val_loss += loss.item()\n",
    "            total_batches += 1.0\n",
    "            \n",
    "            truth = truth.to(\"cpu\")\n",
    "            output = output.to(\"cpu\")\n",
    "            \n",
    "            for i in range(len(truth)):\n",
    "                pred = [a.item() for a in truth[i]]\n",
    "                o = [a.item() for a in output[i]]\n",
    "                o = [1.0 if a > 0.5 else 0.0 for a in o]\n",
    "                ji = calculate_jaccard_index(pred, o)\n",
    "                jaccard_indices.append(ji)\n",
    "#             print(jaccard_indices)\n",
    "            \n",
    "        total_loss = sum_val_loss/total_batches\n",
    "        val_losses.append(total_loss)\n",
    "        average_ji = float(sum(jaccard_indices))/float((batch_size*total_batches))\n",
    "        print(\"In epoch {}, validation loss = {}\".format(epoch, total_loss))\n",
    "        print(\"In epoch {}, average jaccard index = {}\".format(epoch, average_ji))\n",
    "        print()\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, training loss = 2.093218746658874\n",
      "In epoch 0, validation loss = 0.7304236099526689\n",
      "In epoch 0, average jaccard index = 0.8823310810810785\n",
      "\n",
      "In epoch 1, training loss = 0.8008902307650815\n",
      "In epoch 1, validation loss = 0.32049158255796173\n",
      "In epoch 1, average jaccard index = 0.8845270270270245\n",
      "\n",
      "In epoch 2, training loss = 0.4188594299636475\n",
      "In epoch 2, validation loss = 0.24316257439755104\n",
      "In epoch 2, average jaccard index = 0.8924324324324299\n",
      "\n",
      "In epoch 3, training loss = 0.3365693133171291\n",
      "In epoch 3, validation loss = 0.2409886907081346\n",
      "In epoch 3, average jaccard index = 0.894662162162162\n",
      "\n",
      "In epoch 4, training loss = 0.3285357866385212\n",
      "In epoch 4, validation loss = 0.24795932222057032\n",
      "In epoch 4, average jaccard index = 0.8931418918918897\n",
      "\n",
      "In epoch 5, training loss = 0.3270398364083408\n",
      "In epoch 5, validation loss = 0.24160421498723933\n",
      "In epoch 5, average jaccard index = 0.8929054054054016\n",
      "\n",
      "In epoch 6, training loss = 0.32219712579087034\n",
      "In epoch 6, validation loss = 0.24810229402941628\n",
      "In epoch 6, average jaccard index = 0.8956081081081055\n",
      "\n",
      "In epoch 7, training loss = 0.3188824159641788\n",
      "In epoch 7, validation loss = 0.2547891760194624\n",
      "In epoch 7, average jaccard index = 0.8961824324324301\n",
      "\n",
      "In epoch 8, training loss = 0.31451626346535877\n",
      "In epoch 8, validation loss = 0.2440049893952705\n",
      "In epoch 8, average jaccard index = 0.8954054054054041\n",
      "\n",
      "In epoch 9, training loss = 0.31086791133227415\n",
      "In epoch 9, validation loss = 0.2467253896835688\n",
      "In epoch 9, average jaccard index = 0.8957094594594571\n",
      "\n",
      "In epoch 10, training loss = 0.307693237923596\n",
      "In epoch 10, validation loss = 0.2551579129051518\n",
      "In epoch 10, average jaccard index = 0.8936824324324306\n",
      "\n",
      "In epoch 11, training loss = 0.30350098218003363\n",
      "In epoch 11, validation loss = 0.24874397870656606\n",
      "In epoch 11, average jaccard index = 0.8955743243243235\n",
      "\n",
      "In epoch 12, training loss = 0.2980330091429083\n",
      "In epoch 12, validation loss = 0.24752274075069944\n",
      "In epoch 12, average jaccard index = 0.894527027027026\n",
      "\n",
      "In epoch 13, training loss = 0.29405695557186046\n",
      "In epoch 13, validation loss = 0.24555771213931007\n",
      "In epoch 13, average jaccard index = 0.895067567567566\n",
      "\n",
      "In epoch 14, training loss = 0.28572580810279063\n",
      "In epoch 14, validation loss = 0.24751502194920103\n",
      "In epoch 14, average jaccard index = 0.8936486486486475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = PosterClassifier(d1=450, d2=300)\n",
    "# dev = \"cuda:1\"\n",
    "# model = model.to(dev)\n",
    "# for data in traindl:\n",
    "#     images, truth = data\n",
    "#     images = images.float().to(dev)\n",
    "#     output = model(images)\n",
    "#     break\n",
    "model, train_losses, val_losses = train(traindl, valdl, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual = [a.item() for a in truth[0]]\n",
    "# pred = [a.item() for a in output[0]]\n",
    "# pred = [1.0 if a > 0.5 else 0.0 for a in pred]\n",
    "# calculate_jaccard_index(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(train_losses, 'r')\n",
    "plt.plot(val_losses, 'b')\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I briefly played around with the idea of data augmentation and image normalization for this problem, which could have potentially helped a lot, but since the task was to improve the loss by modifying the network architecture only, I didn't do that.\n",
    "\n",
    "Another way to solve this problem could have been to use a pre-trained model on this problem, if one is available - kind of like how we used BERT for sequence classification in Assignment 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run a testing routine\n",
    "\n",
    "Most of the code is kept same here. The loss on the testing data was 1.03 in the original example, so this loss of ~0.24 on the test set through the updated model shows that it has underwent a great improvement. However, I wanted to evaluate it through another metric like accuracy, but found that this doesn't work too well when our output is a one-hot vector. Accuracy seems like too harsh a metric, if we use like how I tried to - but perhaps there are better ways to evaluate a multi-label classification problem than accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model = model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    sumloss = 0\n",
    "    items = 0\n",
    "    total_predictions = 0\n",
    "    accurate_predictions = 0\n",
    "    jaccard_indices = []\n",
    "    for c, data in enumerate(testdl):\n",
    "        images, truth = data\n",
    "        output = model(images.float())\n",
    "        loss = criterion(output, truth)\n",
    "        sumloss += loss\n",
    "        items += 1.0\n",
    "\n",
    "        o = output > 0.8\n",
    "        o = (o.float() == truth)[0]\n",
    "        values = list(set([x.item() for x in o]))\n",
    "        if len(values) == 1 and values[0] is True:\n",
    "            accurate_predictions += 1\n",
    "        total_predictions += 1\n",
    "        \n",
    "        pred = [a.item() for a in output[0]]\n",
    "        pred = [1.0 if a > 0.5 else 0.0 for a in pred]\n",
    "        actual = [a.item() for a in truth[0]]\n",
    "        j_index = calculate_jaccard_index(actual, pred)\n",
    "        jaccard_indices.append(j_index)\n",
    "\n",
    "    print(\"Loss on test data = {}\".format(sumloss/items))\n",
    "    print(\"Accuracy on test data = {}\".format(accurate_predictions/total_predictions))\n",
    "    print(\"Jaccard index on test data = {}\".format(sum(jaccard_indices)/total_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test data = 0.24928903579711914\n",
      "Accuracy on test data = 0.005513439007580979\n",
      "Jaccard index on test data = 0.9101860785665119\n"
     ]
    }
   ],
   "source": [
    "test(model, testdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps another way to use the accuracy metric on this problem is to consider all labels individually? So the accuracy would not be predicted over whether the whole output vector was correct or not, but which elements inside each output vector were predicted accurately. However, that still leaves the problem of the threshold to use when figuring out whether a certain element in the output vector is 1 or 0. Perhaps we can experiment with threshold values using the ROC curve? I'm not sure if that applies here or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note\n",
    "\n",
    "I found this quite an interesting problem, and a good introduction into working with image data, instead of text data as we usually have. I would have liked to try a bunch of other things, since I'm sure there a lot of things we can do with neural networks and image data - however, I do not know much about them (yet!) as we have mostly dealt with text data in this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't quite know why the model performs so poorly; based on the validation loss and the performance on the test data, it seems like the model either doesn't learn much, or it massively overfits in the start. Perhaps it could also be a data quantity issue - ~6000 instances for colored images multi-label classification might not be enough for good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, perhaps the Jaccard similarity might have been a better evaluation metric for this model rather than the Jaccard Index? Since the former penalizes mismatches more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
